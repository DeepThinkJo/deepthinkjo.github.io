---
title: "2 Linear Transformations and Matrices"
date: 2026-02-18
categories: [Fundamentals, Linear Algebra]
tags: [math, linear-algebra, ai, ml, engineering]
math: true
---

# Linear Transformations and Matrices

---

The term "Linear" in Linear Algebra does not merely refer to vectors or matrices; it describes the fundamental properties of **Linear Transformations**. In fact, it is no exaggeration to say that everything in Linear Algebra revolves around vectors and the transformations that govern them. This post introduces the essence of these transformations and their matrix representations.

---

## Definition: Linear Transformation

A mapping $T: V \to W$ between two vector spaces $V$ and $W$ over the same field $\mathbb{F}$ is called a **Linear Transformation** if it satisfies the following two conditions for all $\mathbf{u}, \mathbf{v} \in V$ and $c \in \mathbb{F}$:

1.  **Additivity**: $T(\mathbf{u} + \mathbf{v}) = T(\mathbf{u}) + T(\mathbf{v})$
2.  **Homogeneity**: $T(c\mathbf{u}) = cT(\mathbf{u})$

Linearity essentially means the transformation preserves the algebraic structure of addition and scalar multiplication. This reaffirms that these two operations are the core of Linear Algebra.

---

## Range and Kernel

There are two critical subspaces defined for any linear transformation: the **Range** and the **Kernel**.

### Definition: Range
The **Range** (or Image) of $T$, denoted by $R(T)$, is the set of all vectors in $W$ that are images of vectors in $V$:
$$R(T) = \{ T(\mathbf{v}) \mid \mathbf{v} \in V \}$$

### Definition: Kernel
The **Kernel** (or Null Space) of $T$, denoted by $N(T)$, is the set of all vectors in $V$ that map to the zero vector in $W$:
$$N(T) = \{ \mathbf{v} \in V \mid T(\mathbf{v}) = \mathbf{0}_W \}$$

### Why do we need the Kernel?
While the concept of the **Range** is defined for general functions as well, the **Kernel** holds unique importance in Linear Algebra:
1.  **Significance of the Zero Vector**: The zero vector is a fundamental axiom of any vector space. Identifying which vectors collapse into this anchor reveals the "loss of information" during a transformation.
2.  **Solving Linear Systems**: Any linear equation can be framed as $T(\mathbf{x}) = \mathbf{b}$. For the homogeneous system $T(\mathbf{x}) = \mathbf{0}$, the solution set is precisely the **Kernel**. Thus, the kernel is the set of answers to our linear system.

---

## Rank, Nullity, and the Dimension Theorem

### Definition: Rank and Nullity
- **Rank**: The dimension of the range, $\text{rank}(T) = \dim(R(T))$.
- **Nullity**: The dimension of the kernel, $\text{nullity}(T) = \dim(N(T))$.

### Theorem: The Dimension Theorem (Rank-Nullity Theorem)
Let $V$ be a finite-dimensional vector space and $T: V \to W$ be a linear transformation. Then:
$$\text{rank}(T) + \text{nullity}(T) = \dim(V)$$

#### Intuition
If the domain $V$ has $n$ dimensions, the rank cannot exceed $n$. If more vectors map to the zero vector (higher nullity), fewer "survive" to span the range. This theorem quantifies the trade-off between information preserved and information collapsed into zero.

---

## Transitioning to Computation: Representations

To perform operations via computers, we need to represent abstract transformations as matrices.

### Definition: Coordinate Mapping
For a basis $$\beta = \{ \mathbf{v}_1, \dots, \mathbf{v}_n \}$$, every $$\mathbf{v} \in V$$ can be uniquely represented as a **column vector** $$[\mathbf{v}]_\beta \in \mathbb{F}^n$$.

#### The Significance of Coordinate Mapping
The true power of **Coordinate Mapping** lies in its ability to represent any element of an abstract vector space as a simple sequence of scalars, the length of which is exactly the **Dimension** of the space. This is what we call a **Coordinate System**. 

For example, we often represent a point on the $$xy$$-plane as a tuple $$(x, y)$$. This notation 'implicitly' assumes the standard basis $$\{ (1, 0), (0, 1) \}$$ for $$\mathbb{R}^2$$, meaning $$(x, y)$$ is actually a shorthand for the **Linear Combination**:
$$(x, y) = x(1, 0) + y(0, 1)$$
It is only through coordinate mapping that we can bypass this formal summation and treat abstract vectors as simple, manageable tuples of numbers.

### Definition: Matrix Representation
The **matrix representation** of $T: V \to W$ relative to bases $\beta$ and $\gamma$ is a matrix $[T]_\beta^\gamma$ that allows us to compute the transformation using matrix multiplication.

---

## Isomorphism: The Bridge to Computability

### Definition: Isomorphism
A linear transformation $T: V \to W$ is an **Isomorphism** if it is bijective. If such a mapping exists, $V$ and $W$ are **isomorphic** ($V \cong W$).

Through isomorphism, an abstract vector space can be treated as $\mathbb{F}^n$, and its transformations as matrices.

---

## Deep Thoughts of Jo: Isomorphism as a Tool for Mathematical Modeling

In the world of engineering, **Isomorphism** is much more than a formal definition; it is a supreme tool for **Mathematical Modeling**. It allows us to transform a complex, real-world problem into a computable, simplified form without losing the essential structure.

### Modeling in Machine Learning: Image Flattening
Consider a machine learning model processing images. A computer does not "see" an image; it treats it as a data structure that must be converted into a computable format—usually a **column vector** where each component represents a **feature**.

For a 1-channel image of size $h \times w$, we often use a **Flatten** operation:
$$\{x_{ij} \mid 1 \le i \le h, 1 \le j \le w\} \to \{x_k \mid 1 \le k \le hw\}$$

Strictly speaking, pixels are integers in $[0, 255]$, but we treat them as elements of a vector space for linear operations. This flattening process acts as an **Isomorphism** between the 2D image space and the 1D vector space.

### The Intuition of 'Slots'
What matters here is that both spaces are finite-dimensional and share the same dimension ($hw$). While the arrangement (the "array style") differs, their **intrinsic structure** is identical.

In this context, I like to think of **Dimension** as the number of **'slots'** required to uniquely specify an element within a set. Whether these slots are arranged in a grid (Image) or a single column (Flattened Vector), the number of slots remains constant. As long as the number of slots—the dimension—is preserved, isomorphism ensures that the problem we solve in the simplified vector space is perfectly equivalent to the original problem.

### Conclusion
Abstract spaces are for human intuition, but **Euclidean spaces are for machines**. By identifying the underlying structure and using isomorphism to map data into $n$-tuples, we drastically reduce the difficulty of our problems. Isomorphism is the bridge that allows us to turn complex signals and images into simple, computable arithmetic.

---
**References**
* *DeepThinkJo's Archive: Foundations of AI - Linear Algebra (2)*
