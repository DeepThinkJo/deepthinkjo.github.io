---
title: "Linear Algebra (4): Linear Systems"
date: 2026-02-19
categories: [Fundamentals, Linear Algebra]
tags: [math, linear-algebra, ai, ml, engineering, linear-systems, gaussian-elimination, row-reduction, dimension]
math: true
---

# Linear Systems

A **Linear System** is essentially a collection of linear equations involving multiple unknowns. While the definition sounds simple, its implications are profound. Through the process of mathematical modeling, complex real-world problems—ranging from structural engineering to neural network optimizations—can be translated into these systems. As engineers, mastering linear systems is not just an academic exercise; it is the acquisition of a vital tool for solving the problems of reality.

---

## Linear System and the Nature of Solution Sets

To solve a problem, we must first define it clearly. A linear system is our framework for defining relationships between variables.

### Definition: Linear System
A system of $m$ linear equations in $n$ variables $x_1, x_2, \dots, x_n$ is defined as:
$$
\begin{cases}
a_{11}x_1 + a_{12}x_2 + \dots + a_{1n}x_n = b_1 \\
a_{21}x_1 + a_{22}x_2 + \dots + a_{2n}x_n = b_2 \\
\vdots \\
a_{m1}x_1 + a_{m2}x_2 + \dots + a_{mn}x_n = b_m
\end{cases}
$$

When we treat a linear system as a problem to be solved, our primary objective is to find its **Solution**.

### Definition: Solution and Solution Set
A **solution** of a linear system is a list of numbers $(s_1, s_2, \dots, s_n)$ that makes each equation in the system a true statement. The **solution set** is the collection of all such possible solutions.

Interestingly, because these equations possess the property of **linearity**, their solution sets behave in a very specific manner. Unlike other types of equations that might have a finite number of solutions (like two or three), a linear system follows a strict rule.

### Theorem: The Three Possibilities
A linear system has:
1. **No solution** (Inconsistent)
2. **Exactly one solution** (Consistent)
3. **Infinitely many solutions** (Consistent)

If a solution exists (cases 2 or 3), we call the system **consistent**; otherwise, it is **inconsistent**.

---

## Two Perspectives: Row Picture and Column Picture

Depending on how we look at a linear system, we gain different levels of insight. We can categorize these into the **Row Picture** and the **Column Picture**.

### 1. The Row Picture: Focusing on Individual Equations
In the Row Picture, we view the system as a collection of equations. Geometrically, each equation represents a hyperplane, and finding the solution set means finding the **intersection** of all these hyperplanes. This view is highly intuitive for manual calculation and forms the basis for algorithms like Gaussian Elimination.

### 2. The Column Picture: Focusing on Vector Operations
While the Row Picture is useful for solving the system, we can unlock much deeper "Linear Algebra insights" by transforming the system into a different, yet equivalent, form.

**Theorem: Equivalence to Vector Equations**
The solution set of the system of equations is identical to the set of scalars $x_1, \dots, x_n$ that satisfy the following vector equation:
$$x_1 \mathbf{a}_1 + x_2 \mathbf{a}_2 + \dots + x_n \mathbf{a}_n = \mathbf{b}$$
where $\mathbf{a}_i$ represents the $i$-th column of the coefficient matrix.

Through this theorem, we can rephrase our problem: Can the vector $\mathbf{b}$ be expressed as a **linear combination** of the vectors $\{\mathbf{a}_1, \dots, \mathbf{a}_n\}$? This perspective allows us to analyze the system's solvability using concepts like *Spanning* and *Linear Independence* without even having to solve the system directly.

In summary, the **Row Picture** is more practical for finding the actual solution set, while the **Column Picture** is indispensable for analyzing the structural properties of the system through the lens of linear algebra.

---

## Deep Thoughts of Jo: The Necessity of Elimination Algorithms

Let’s dive into the Row Picture and discuss how to solve these systems. You might think, *"I already know how to solve equations from high school."* But there are two critical motivations for learning formal **Elimination Algorithms**.

**1. Scalability and Computational Efficiency**
Solving a system with two variables is easy. But what if we have 1,000 equations and 1,000 unknowns? To prevent human error, ensure efficiency, and allow computers to handle the workload, we need a clear, systematic, and algorithmic approach. This is the first reason we need Gaussian or Gauss-Jordan elimination.

**2. Breaking the "Equation Count" Intuition**
If you have a good mathematical intuition, you might be tempted to say:
- *"If I have $n$ unknowns and $m$ equations, then if $n > m$, there are infinite solutions. If $m > n$, there is no solution. If $m = n$, there is a unique solution."*

Unfortunately, this intuition is **often wrong**. Consider this simple counter-example:
$$x + y = 1$$
$$x + y = 2$$
Even though the number of unknowns equals the number of equations, there is no solution because the equations are contradictory. Elimination algorithms serve as a filter that transforms a system into an equivalent form where these "traps" are revealed, allowing us to accurately determine the size of the solution set.

---

## Row Picture: The Mechanics of Row Reduction

To apply the Row Picture systematically, we transition to matrix notation.

### Definition: Coefficient Matrix and Augmented Matrix
By converting the system into a matrix, we not only simplify the representation but also gain access to matrix operations. The **Coefficient Matrix** contains only the $a_{ij}$ terms, while the **Augmented Matrix** includes the $b$ vector.

### Definition: Elementary Row Operations (ERO)
The core of elimination lies in performing operations that **do not change the solution set**:
1. Replacement
2. Interchange
3. Scaling

### Definition: REF and RREF
Our goal is to reach the **Row Echelon Form (REF)** or **Reduced Row Echelon Form (RREF)**. In these forms, the augmented matrix becomes "transparent," making it trivial to find the solution set.

**Algorithm: Row Reduction (Gaussian Elimination)**
1. **Find Pivot**: Locate the leftmost non-zero column.
2. **Position Pivot**: Move a non-zero entry to the pivot position.
3. **Eliminate Below**: Create zeros in all positions below the pivot.
4. **Repeat**: Repeat for the remaining sub-matrix until REF is reached.
5. **Backward Phase**: Create zeros above each pivot and scale pivots to 1 (Gauss-Jordan).



---

## Column Picture: Structural Insights

The Column Picture $Ax = b$ reveals existence and uniqueness:

**Theorem: Existence and Uniqueness**
- If the columns of $A$ **span** $\mathbb{R}^m$, then $Ax = b$ is **consistent** for every $\mathbf{b}$.
- If the columns of $A$ are **linearly independent**, the solution (if it exists) is **unique**.

---

## Deep Thoughts of Jo: Dimension as the Boundary of Logic

In our first post on Vector Spaces, we briefly touched upon **Dimension**. Now, with the context of linear systems, the true significance of dimension becomes clear.

Dimension is the **critical boundary** of a vector space. It is the **maximum** number of elements that can be linearly independent, and simultaneously, the **minimum** number of elements required to span the space. This duality allows us to infer the properties of a set of vectors simply by counting them.
- If a set has fewer elements than the dimension, it **cannot span** the space.
- If a set has more elements than the dimension, it **must be linearly dependent**.

Let’s apply this "counting logic" to an $m \times n$ linear system $Ax = b$:

**1. Case $m > n$ (Tall Matrix)**
* **Row Picture**: We suspect the system is **overdetermined** (too many constraints).
* **Column Picture**: Since the number of columns ($n$) is less than the dimension of the target space $\mathbb{R}^m$, the columns **cannot span** $\mathbb{R}^m$. Therefore, there will always be some vectors $\mathbf{b}$ for which no solution exists.

**2. Case $m < n$ (Wide Matrix)**
* **Row Picture**: We suspect the system is **underdetermined** (not enough constraints).
* **Column Picture**: Since the number of columns ($n$) exceeds the dimension of $\mathbb{R}^m$, the columns **must be linearly dependent**. This redundancy implies that if the system is consistent, there is a very high probability of having **infinitely many solutions**.

By understanding dimension as this "boundary," we can predict the behavior of a linear system just by glancing at its shape ($m \times n$), even before we begin the first step of row reduction.

---

## Homogeneous and Nonhomogeneous Systems

Finally, we highlight the beautiful connection between linear systems and linear transformations.

### Definition: Homogeneous vs. Nonhomogeneous
A system is **Homogeneous** if $Ax = 0$. Otherwise, it is **Nonhomogeneous** ($Ax = b$).

The solution set of $Ax = 0$ corresponds to the **Kernel** (or **Null Space**) of the transformation $A$. Since the kernel is a subspace, the solution set of $Ax = 0$ is a vector space, which we call the **Solution Space**.

### Theorem: General Solution Structure
If $\mathbf{x}_p$ is a **particular solution** to $Ax = b$, and $\mathbf{x}_h$ is the general solution to $Ax = 0$, then:
$$\mathbf{x} = \mathbf{x}_p + \mathbf{x}_h$$

This means the entire solution set of a nonhomogeneous system is just a **shifted version** of the Null Space. Once you find one specific point ($\mathbf{x}_p$), you can find all others by traversing the vectors in the Null Space.

---
**References**

1. Stephen H. Friedberg, Arnold J. Insel, Lawrence E. Spence, *Linear Algebra*, 5th ed.
2. Erwin Kreyszig, *Advanced Engineering Mathematics*, 10th ed.
3. David C. Lay, Steven R. Lay, Judi J. McDonald, *Linear Algebra and Its Applications*, 5th ed.
