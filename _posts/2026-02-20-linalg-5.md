---
title: "Linear Algebra (5): Determinants - Into the Numerical World"
date: 2026-02-21
categories: [Fundamentals, Linear Algebra]
tags: [math, linear-algebra, ai, ml, engineering, determinants, recursion, complexity, big-o, geometry]
math: true
---

# Determinants

A **Determinant** is a scalar value defined for every square matrix. While it might not be the most frequently used tool in massive-scale practical applications, it is a concept of immense theoretical beauty. For an engineer, understanding the determinant is the key to unlocking the secrets of matrix invertibility and the geometry of transformations.

---

## Definition: Determinant

We define the determinant of an $n \times n$ matrix $A$ using **Laplace Expansion** (also known as cofactor expansion). 

### 1. Case $n = 1$ (Base Case)
For a $1 \times 1$ matrix $A = [a_{11}]$, the determinant is simply the value of the entry itself:
$$\det(A) = a_{11}$$

### 2. Case $n \ge 2$ (Recursive Step)
For a matrix of order $n \ge 2$, the determinant is defined as the sum of the products of the entries of any row (or column) and their corresponding cofactors. For example, expanding along the $i$-th row:

$$
\det(A) = \sum_{j=1}^{n} a_{ij} C_{ij} = a_{i1}C_{i1} + a_{i2}C_{i2} + \dots + a_{in}C_{in}
$$

where $C_{ij} = (-1)^{i+j} \det(M_{ij})$ is the **cofactor**, and $M_{ij}$ is the submatrix created by deleting the $i$-th row and $j$-th column.

---

## Deep Thoughts of Jo: Determinant is a Recursive Function

In Computer Science, we study **Algorithms** to find systematic solutions to problems. One of the most powerful concepts in this field is the **Recursive Function**.

### Definition: Recursive Function
A recursive function is a function that calls itself to solve a smaller version of the same problem. It consists of:
1.  **Base Case**: The simplest possible instance of the problem that can be answered directly (e.g., $n=1$).
2.  **Recursion**: The process where the function calls itself with a reduced input (e.g., $n \ge 2$ calling sub-determinants).

The definition of a determinant perfectly mirrors a recursive function. Connecting abstract mathematical definitions with CS concepts like recursion is a vital process for any **engineer or researcher**. These cross-disciplinary links are where new inspirations and breakthroughs often happen.

---

## Determinants and EROs

To understand more advanced methods for solving systems and finding inverses, we must know how **Elementary Row Operations (EROs)** affect the determinant.

### Theorem: Row Operations and Determinants
1.  **Replacement**: Adding a multiple of one row to another **does not change** the determinant.
2.  **Interchange**: Swapping two rows **multiplies the determinant by $-1$**.
3.  **Scaling**: Multiplying a row by a non-zero scalar $k$ **multiplies the determinant by $k$**.

Crucially, while EROs can change the value, they **never** change whether the determinant is zero or non-zero. This leads to a fundamental theorem:

### Theorem: The Invertibility Criterion
A square matrix $A$ is **invertible** if and only if $\det(A) \neq 0$.

---

## Comparing Methods for Linear Systems ($A\mathbf{x} = \mathbf{b}$)

### 1. Gaussian or Gauss-Jordan Elimination
The standard algorithmic approach. Reliable, systematic, and the basis for most row-based calculations.

### 2. LU Decomposition
Factorizing $A$ into $L$ and $U$. This is particularly efficient when solving $A\mathbf{x} = \mathbf{b}$ for many different $\mathbf{b}$ vectors using the same $A$.

### 3. $\mathbf{x} = A^{-1}\mathbf{b}$
Solving by multiplying the inverse. Requires $A$ to be invertible and is often **numerically unstable**.

### 4. Cramer's Rule
Uses determinants to find each variable $x_i$. While mathematically elegant, it requires calculating $(n+1)$ separate determinants, making it very costly for large systems.

---

## Comparing Methods for Inverse Matrices ($A^{-1}$)

### 1. Gauss-Jordan Elimination
Applying row reduction to the augmented matrix $[A | I]$ to reach $[I | A^{-1}]$.

### 2. Adjugate Matrix (The Classical Adjoint Method)
**Theorem**: If $A$ is invertible, $A^{-1} = \frac{1}{\det(A)} \text{adj}(A)$.

---

## Comparing Methods for Determinants

### 1. Laplace Expansion
Direct application of the definition. Extremely inefficient for large $n$ ($O(n!)$).

### 2. LU Decomposition
If we decompose $A = LU$ such that $L$ has only $1$s on its main diagonal, then $\det(A)$ is the **product of the entries on the main diagonal of $U$**:

$$
\det(A) = \prod_{i=1}^{n} u_{ii}
$$

---

## Deep Thoughts of Jo: Into the Numerical World

As engineers, our job isn't just to find \"a\" solution, but the **most efficient** one. In the modern age of AI, where hardware like GPUs is essential, using inefficient algorithms is a catastrophic waste of resources. 

We measure performance using **Time Complexity**, represented by **Big-O Notation**. Below is a comparison of the complexities for the methods we've discussed.



### Time Complexity Comparison ($n \times n$ Matrix)

| Task | Method | Time Complexity | Note |
| :--- | :--- | :--- | :--- |
| **Linear System** | Gaussian / LU | $O(n^3)$ | Most efficient and stable. |
| | Cramer's Rule | $O(n^4)$ | Assumes LU for det; $O(n!)$ if Laplace. |
| | Inverse Matrix | $O(n^3)$ | Numerically unstable. |
| **Inverse Matrix** | Gauss-Jordan | $O(n^3)$ | Standard approach. |
| | Adjugate Matrix | $O(n^5)$ | Assumes LU for det; $O(n!)$ if Laplace. |
| **Determinant** | LU Decomposition | $O(n^3)$ | Practical and fast. |
| | Laplace Expansion | $O(n!)$ | Theoretically recursive, practically slow. |

In AI, where $n$ can be in the millions, choosing $O(n!)$ or $O(n^5)$ is a death sentence for your project. Always prioritize the right tool for the scale.

---

## Determinant as Volume Ratio

While vectors in linear algebra are generalized objects, the best way to understand them is to consider specific cases—namely, the **geometric perspective**. By limiting our view to Euclidean space $\mathbb{R}^n$ (where $n \le 3$), we can visualize how these concepts operate.

Unlike previous discussions, let’s consider the input of a linear transformation not as a single vector, but as a **region** (a set of vectors). The output of this region under a transformation $T(\mathbf{x}) = A\mathbf{x}$ could be a point, a line, a plane, or a higher-dimensional space depending on the rank of $A$.

### The Geometric Meaning
Consider a transformation $T: \mathbb{R}^2 \to \mathbb{R}^2$ defined by $T(\mathbf{x}) = A\mathbf{x}$. If we take a region $S$ in the domain with a certain area, the determinant of $A$ represents the **ratio** of the area of the transformed region $T(S)$ to the original area:

$$
|\det(A)| = \frac{\text{Area}(T(S))}{\text{Area}(S)}
$$



If $\det(A) = 0$, it means the area collapses to zero (e.g., a plane squashed into a line or a point), indicating that the transformation is not invertible.

This concept extends to $\mathbb{R}^3$, where the ratio applies to **volumes**. For $\mathbb{R}^n$ where $n \ge 4$, though we cannot visualize it, the determinant still represents the ratio of the **$n$-dimensional volume (hypervolume)**. 

### Deep Thoughts of Jo: Why Only Square Matrices?
This geometric intuition allows us to reconfirm why the determinant is only defined for **square matrices**. If the dimensions of the domain and codomain were different (e.g., $T: \mathbb{R}^2 \to \mathbb{R}^3$), we would be comparing a 2D "area" to a 3D "volume." Since these "volumes" are defined in fundamentally different ways across dimensions, a single ratio (the determinant) cannot exist. Re-evaluating definitions through such conceptual meanings is a powerful way to deepen one's mathematical maturity.

---

**References**

1. Stephen H. Friedberg, Arnold J. Insel, Lawrence E. Spence, *Linear Algebra*, 5th ed.
2. Erwin Kreyszig, *Advanced Engineering Mathematics*, 10th ed.
3. David C. Lay, Steven R. Lay, Judi J. McDonald, *Linear Algebra and Its Applications*, 5th ed.
