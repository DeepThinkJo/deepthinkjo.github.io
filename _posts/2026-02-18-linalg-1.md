---
title: "Fundamentals - Linear Algebra (1): Vector Spaces"
date: 2026-02-18
categories: [Fundamentals, Linear Algebra]
tags: [math, linear-algebra, ai, ml]
math: true
---

# Vector Spaces

---

In linear algebra, we primarily study vectors and matrices.  
Therefore, we must first understand what a vector truly is.

However, the vector discussed in linear algebra is not:

- the arrow used in physics (a quantity with both magnitude and direction), nor  
- the array used in computer science (a sequence of numeric values).

Linear algebra defines vectors by **generalizing** the common structural properties shared by these seemingly different objects.

In many areas of mathematics, generalization is achieved through axioms.  
Linear algebra follows this same principle.

Yet we cannot define a vector immediately.  
Before defining vectors, we must first define a field, scalars, and a vector space.

---

## Definition of a Field

A **field** $\mathbb{F}$ is a set equipped with two operations, addition and multiplication, such that:

1. $(\mathbb{F}, +)$ forms an abelian group.
2. $(\mathbb{F} \setminus \{0\}, \cdot)$ forms an abelian group.
3. Multiplication is distributive over addition:

$$
a(b + c) = ab + ac
$$

for all $a, b, c \in \mathbb{F}$.

The elements of a field are called **scalars**.

---

## Definition of a Vector Space

Let $\mathbb{F}$ be a field.  
A **vector space** $V$ over $\mathbb{F}$ is a set equipped with two operations:

- vector addition: $V \times V \to V$
- scalar multiplication: $\mathbb{F} \times V \to V$

such that for all $\mathbf{u}, \mathbf{v}, \mathbf{w} \in V$ and $a, b \in \mathbb{F}$, the following axioms hold:

1. $\mathbf{u} + \mathbf{v} = \mathbf{v} + \mathbf{u}$
2. $(\mathbf{u} + \mathbf{v}) + \mathbf{w} = \mathbf{u} + (\mathbf{v} + \mathbf{w})$
3. There exists a zero vector $\mathbf{0} \in V$ such that $\mathbf{v} + \mathbf{0} = \mathbf{v}$
4. For every $\mathbf{v} \in V$, there exists $-\mathbf{v} \in V$ such that $\mathbf{v} + (-\mathbf{v}) = \mathbf{0}$
5. $a(\mathbf{u} + \mathbf{v}) = a\mathbf{u} + a\mathbf{v}$
6. $(a + b)\mathbf{v} = a\mathbf{v} + b\mathbf{v}$
7. $a(b\mathbf{v}) = (ab)\mathbf{v}$
8. $1\mathbf{v} = \mathbf{v}$

---

From this definition, we see that any set satisfying these axioms qualifies as a vector space.  
Furthermore, addition and scalar multiplication need not be the familiar operations we already know — they may be defined differently.

However, if one studies linear algebra for AI and machine learning,  
it is more important to understand that the essence of a vector space lies in **addition and scalar multiplication**,  
rather than in verifying axioms for arbitrary sets.

In AI and machine learning, the most commonly encountered vector space is the Euclidean space.  
Nevertheless, several other examples are equally important.

---

### Euclidean Space ($\mathbb{F}^n$)

$\mathbb{F}^n$ is the set of all ordered $n$-tuples

$$
(x_1, x_2, \dots, x_n)
$$

where each $x_i \in \mathbb{F}$.

An element of $\mathbb{F}^n$ is called a **vector**, typically written as

$$
\mathbf{x}
=
\begin{bmatrix}
x_1 \\
x_2 \\
\vdots \\
x_n
\end{bmatrix}
$$

or equivalently $\mathbf{x} = (x_1, \dots, x_n)^T$.

Vector addition and scalar multiplication are defined componentwise:

$$
\mathbf{x} + \mathbf{y}
=
(x_1 + y_1, \dots, x_n + y_n)
$$

$$
a\mathbf{x}
=
(ax_1, \dots, ax_n)
$$

This is the most familiar vector space in AI and ML,  
where data samples, feature representations, and model parameters are represented as vectors in $\mathbb{R}^n$.

---

### Matrix Space ($\mathbb{F}^{m \times n}$)

$\mathbb{F}^{m \times n}$ is the set of all $m \times n$ matrices with entries in $\mathbb{F}$.

A matrix is written as

$$
\mathbf{A}
=
\begin{bmatrix}
a_{11} & \cdots & a_{1n} \\
\vdots & \ddots & \vdots \\
a_{m1} & \cdots & a_{mn}
\end{bmatrix}
$$

Matrix addition and scalar multiplication are defined entrywise:

$$
(\mathbf{A} + \mathbf{B})_{ij} = A_{ij} + B_{ij}
$$

$$
(a\mathbf{A})_{ij} = a A_{ij}
$$

Thus matrices themselves form a vector space.

In machine learning, weight matrices, covariance matrices, and linear transformations all belong to such matrix spaces.

---

### Polynomial Space ($\mathbb{P}_n$)

$\mathbb{P}_n$ is the set of all polynomials of degree at most $n$:

$$
p(x) = a_0 + a_1 x + \dots + a_n x^n
$$

Addition and scalar multiplication are defined coefficientwise.

Although polynomials look very different from column vectors,  
they satisfy the same vector space axioms.

---

### Function Space

Let $X$ be a set.  
The set of all functions $f : X \to \mathbb{F}$ forms a vector space under pointwise operations:

$$
(f + g)(x) = f(x) + g(x)
$$

$$
(af)(x) = a f(x)
$$

---

Through generalization, polynomials and functions can also be treated as vectors.  
A vector space defined by axioms in this way is called an abstract (or general) vector space.

---

Have we not learned, in set theory, that after defining a set, we proceed to define its subsets?

Linear algebra follows the same pattern.  
For a given vector space, we define the notion of a subspace.

---

### Definition of a Subspace

Let $V$ be a vector space over a field $\mathbb{F}$.  
A subset $W \subseteq V$ is called a **subspace** of $V$ if:

1. $\mathbf{0} \in W$
2. For all $\mathbf{u}, \mathbf{v} \in W$, $\mathbf{u} + \mathbf{v} \in W$
3. For all $a \in \mathbb{F}$ and $\mathbf{v} \in W$, $a\mathbf{v} \in W$

A subspace is itself a vector space under the same operations defined on $V$.

---

The key observation here is that a subspace is itself a vector space.  
In other words, within a given vector space, certain subsets preserve the entire algebraic structure.

In many areas of mathematics, when a structure is referred to as a "space,"  
its subsets often inherit the same fundamental properties.

For example, in probability theory, a subset of a sample space is an event.  
Conditioning on an event effectively treats that event as a new sample space.

Similarly, in linear algebra, subspaces preserve the essential structure of the original space.

---

In many mathematical subjects, once a structure is defined, the next step is often to study combinations.

We study combinations because they allow us to construct more complex objects from simpler ones.

In algebra, once polynomials are defined, we build more complicated functions through sums and products.

In linear algebra, we define the notion of a **linear combination**.

---

### Definition of a Linear Combination

Given vectors $\mathbf{v}_1, \dots, \mathbf{v}_k \in V$ and scalars $a_1, \dots, a_k \in \mathbb{F}$,  
a vector of the form

$$
a_1 \mathbf{v}_1 + a_2 \mathbf{v}_2 + \dots + a_k \mathbf{v}_k
$$

is called a **linear combination** of $\mathbf{v}_1, \dots, \mathbf{v}_k$.

---

Why do we define linear combinations?

Not merely to add vectors together,  
but to **generate new vectors** from vectors we already understand.

This naturally leads to the following question:

What is the full set of vectors that can be expressed as linear combinations of a given collection of vectors?

This set is called the **span**.

---

### Definition of a Span

The **span** of vectors $\mathbf{v}_1, \dots, \mathbf{v}_k$ is the set

$$
\text{span}\{\mathbf{v}_1, \dots, \mathbf{v}_k\}
=
\{ a_1 \mathbf{v}_1 + \dots + a_k \mathbf{v}_k \mid a_i \in \mathbb{F} \}.
$$

---

The span is not merely a set.  
It describes how much of the space we can generate using a given collection of vectors.

However, we can go further.

Suppose we want to express **every** vector in a vector space as a linear combination of certain vectors.

That alone is not sufficient.

If a vector can be represented in multiple different ways,  
the representation becomes redundant and inefficient.

Thus, we desire two properties simultaneously:

- The set must generate the entire space (spanning).
- The representation must be unique (linear independence).

A set satisfying both properties is called a **basis**.

---

### Definition of Spanning

A set of vectors $\{\mathbf{v}_1, \dots, \mathbf{v}_k\}$ is said to **span** a vector space $V$ if

$$
V = \text{span}\{\mathbf{v}_1, \dots, \mathbf{v}_k\}.
$$

---

### Definition of Linear Independence

Vectors $\mathbf{v}_1, \dots, \mathbf{v}_k$ are **linearly independent** if

$$
a_1 \mathbf{v}_1 + \dots + a_k \mathbf{v}_k = \mathbf{0}
$$

implies

$$
a_1 = \dots = a_k = 0.
$$

---

### Definition of a Basis

A subset $B = \{\mathbf{v}_1, \dots, \mathbf{v}_k\}$ of a vector space $V$ is called a **basis** if

1. $B$ spans $V$
2. $B$ is linearly independent

---

A vector space may have multiple different bases.  
However, the number of elements in any basis is always the same.

This invariant quantity reflects the intrinsic structure of the vector space and is called its dimension.

---

### Definition of a Dimension

The **dimension** of a vector space $V$ is the number of elements in any basis of $V$.

---

Below, we examine the bases and dimensions of several familiar vector spaces.

---

### $\mathbb{F}^n$

Let

$\mathbf{e}_1 = (1,0,\ldots,0)$,  
$\mathbf{e}_2 = (0,1,\ldots,0)$,  
$\ldots$,  
$\mathbf{e}_n = (0,0,\ldots,1)$.

A basis for $\mathbb{F}^n$ is

$$
\{\mathbf{e}_1, \mathbf{e}_2, \ldots, \mathbf{e}_n\}.
$$

Since the basis has $n$ elements,

$$
\dim(\mathbb{F}^n) = n.
$$

---

### $\mathbb{F}^{m \times n}$

Let $\mathbf{E}_{ij} \in \mathbb{F}^{m \times n}$ be the matrix with $1$ at position $(i,j)$ and $0$ elsewhere.

A basis for $\mathbb{F}^{m \times n}$ is

$$
\{\mathbf{E}_{ij} \mid 1 \le i \le m,\; 1 \le j \le n\}.
$$

Since the basis has $mn$ elements,

$$
\dim(\mathbb{F}^{m \times n}) = mn.
$$

---

### $\mathbb{P}_n$

A basis for $\mathbb{P}_n$ is

$$
\{1, x, \ldots, x^n\}.
$$

Since the basis has $n+1$ elements,

$$
\dim(\mathbb{P}_n) = n+1.
$$

---

The bases shown above are the simplest and most natural choices.  
Such bases are called **standard bases**.

---

We previously restricted polynomial spaces to degree at most $n$.  
But what if we consider the set of all polynomials without any degree bound?

---

### Definitions of Finite-Dimensional and Infinite-Dimensional Vector Spaces

A vector space $V$ is called **finite-dimensional** if it has a basis consisting of finitely many vectors.

If $V$ does not have a finite basis, then it is called **infinite-dimensional**.

---

Let $\mathbb{P}$ denote the set of all polynomials.  
Its standard basis is

$$
\{1, x, x^2, x^3, \ldots\}.
$$

Since this set is infinite,  
$\mathbb{P}$ is an infinite-dimensional vector space.

---

Does every vector space — finite-dimensional or infinite-dimensional — have a basis?

---

### Theorem

Every vector space has a basis.

---

The proof of this theorem is omitted.

What matters is the existence of a basis,  
no matter how large or complex the vector space may be.

---

## Deep Thoughts of Jo: Taylor Series and the Basis of Function Space

While studying the theorem that **"Every vector space has a basis,"** I began to ponder:  
*"What would the basis of a Function Space look like?"*

A basis must be able to represent every element in the space through a linear combination. While searching for such a set, the concept of the **Taylor Series** from Calculus came to mind. My thought process was as follows:

1. Every vector space must have a basis.
2. For a function space, the basis should be able to express any function through addition and scalar multiplication.
3. I realized that any function that is infinitely differentiable at a point can be expressed as a **Taylor Expansion**.
4. Since a Taylor expansion is essentially a linear combination of power functions, for the **subspace** of analytic functions, the basis would be identical to that of a **Polynomial Space**:
   $$\mathcal{B} = \{1, x, x^2, x^3, \dots\}$$



### Expanding the Connection
This realization bridges the gap between the abstract axioms of linear algebra and the practical tools of calculus. However, to be mathematically precise, two additional perspectives are worth noting:

* **Topological Vector Spaces**: In standard linear algebra, a basis typically deals with *finite* linear combinations. To handle the "infinite sum" of a Taylor series, we require the framework of topological vector spaces, where convergence is defined.
* **Fourier Series**: Similarly, trigonometric functions like $\{\sin(nx), \cos(nx)\}$ can serve as a basis for periodic functions. This is a fundamental concept in signal processing and spectral analysis.

By connecting these dots, I’ve found that the **Generalization** inherent in linear algebra provides a powerful structure for understanding even the most complex function spaces we encounter in engineering.
