---
title: "Linear Algebra (7): Orthogonality"
date: 2026-02-21
categories: [Fundamentals, Linear Algebra]
tags: [math, linear-algebra, ai, ml, engineering, inner-product, orthogonality, gram-schmidt, least-squares, svd]
math: true
---

# Orthogonality

In the realm of Machine Learning, **Orthogonality** is perhaps the most utilized concept of linear algebra. It serves as the core for understanding Singular-Value Decomposition (SVD), the Least-Squares Problem, and many other techniques directly tied to modern AI.

---

## Inner Product Spaces

Beyond addition and scalar multiplication, a vector space where an **inner product** operation is defined is called an **Inner Product Space**. Just as we moved from abstract vector spaces to $\mathbb{R}^n$, we will look at how the abstract axioms of inner products manifest in the Euclidean space as the **Dot Product**.

### Definition: Inner Product Space
An inner product on a vector space $V$ is a function that associates each pair of vectors $\mathbf{u}, \mathbf{v}$ with a scalar $\langle\mathbf{u}, \mathbf{v}\rangle$, satisfying:
1.  **Symmetry**: $\langle\mathbf{u}, \mathbf{v}\rangle = \langle\mathbf{v}, \mathbf{u}\rangle$
2.  **Additivity**: $\langle\mathbf{u} + \mathbf{v}, \mathbf{w}\rangle = \langle\mathbf{u}, \mathbf{w}\rangle + \langle\mathbf{v}, \mathbf{w}\rangle$
3.  **Homogeneity**: $\langle c\mathbf{u}, \mathbf{v}\rangle = c\langle\mathbf{u}, \mathbf{v}\rangle$
4.  **Positivity**: $\langle\mathbf{u}, \mathbf{u}\rangle \ge 0$, and $\langle\mathbf{u}, \mathbf{u}\rangle = 0$ iff $\mathbf{u} = \mathbf{0}$.

### Definition: Norms and the Dot Product
In $\mathbb{R}^n$, the standard inner product is the **Dot Product**:
$$\mathbf{u} \cdot \mathbf{v} = \mathbf{u}^T \mathbf{v} = \sum_{i=1}^n u_i v_i$$
The **Norm** (length) of a vector is defined using the inner product: $\|\mathbf{u}\| = \sqrt{\mathbf{u} \cdot \mathbf{u}}$.

### $L_p$ Norms in Machine Learning
Different ways of measuring "distance" lead to different norms, which are crucial in ML regularization:
* **$L_1$ Norm (Manhattan):** $\|\|\mathbf{x}\|\|_1 = \sum{|x_i|}$
* **$L_2$ Norm (Euclidean):** $\|\|\mathbf{x}\|\|_2 = \sqrt{\sum x_i^2}$
* **$L_\infty$ Norm (Maximum):** $\|\|\mathbf{x}\|\|_{\infty} = \max{|x_i|}$

---

## Orthogonality: The Geometry of Zero

The dot product carries deep geometric information about the angle $\theta$ between vectors.

### Theorem: The Geometric Dot Product
$$\mathbf{u} \cdot \mathbf{v} = \|\mathbf{u}\| \|\mathbf{v}\| \cos(\theta)$$

This leads to the definition of orthogonality: two vectors are **orthogonal** if their dot product is zero ($\theta = 90^\circ$).
* **Orthogonal Set**: A set where every pair of distinct vectors is orthogonal.
* **Orthonormal Set**: An orthogonal set where every vector has a norm of 1.

---

## Orthogonal Matrices

An $n \times n$ matrix $U$ is an **Orthogonal Matrix** if its columns form an orthonormal basis for $\mathbb{R}^n$.

### Definition: Orthogonal Matrix
A square matrix $U$ is orthogonal if $U^T U = I$, which implies $U^T = U^{-1}$.

> **Engineering Note**: Even though the columns must be *orthonormal*, we call it an *orthogonal matrix*. These matrices are "distance-preserving" (isometries); they rotate or reflect space without stretching it:
> $\|U\mathbf{x}\| = \|\mathbf{x}\|$ and $(U\mathbf{x}) \cdot (U\mathbf{y}) = \mathbf{x} \cdot \mathbf{y}$

---

## Orthogonal Complements

If $V$ is a subspace of $\mathbb{R}^n$, its **Orthogonal Complement** $V^\perp$ is the set of all vectors orthogonal to every vector in $V$.
* **Direct Sum**: Any vector in $\mathbb{R}^n$ can be uniquely decomposed as $\mathbf{x} = \mathbf{v} + \mathbf{w}$, where $\mathbf{v} \in V$ and $\mathbf{w} \in V^\perp$.

---

## The Gram-Schmidt Process

How do we turn a "messy" basis into a "clean" orthogonal basis? We use the **Gram-Schmidt Process**. 
The intuition is simple: pick vectors one by one and subtract their projections onto the space spanned by the previously selected vectors.

$$\mathbf{u}_k = \mathbf{v}_k - \sum_{j=1}^{k-1} \text{proj}_{\mathbf{u}_j}(\mathbf{v}_k)$$

---

## Least-Squares Problems: The Heart of Linear Regression

When a linear system $A\mathbf{x} = \mathbf{b}$ has no solution (inconsistent), we look for the "best possible" $\hat{\mathbf{x}}$ that minimizes the error $\|\mathbf{b} - A\mathbf{x}\|$.

In **Supervised Learning**, if $X$ is our feature matrix and $\mathbf{y}$ is the label vector, finding the parameters $\theta$ for a linear model is a least-squares problem. Geometrically, $A\hat{\mathbf{x}}$ is the **orthogonal projection** of $\mathbf{b}$ onto the column space of $A$.

### The Normal Equation
To solve for $\hat{\mathbf{x}}$, we solve:
$$A^T A \hat{\mathbf{x}} = A^T \mathbf{b}$$

---

## Singular Value Decomposition (SVD)

While Eigendecomposition requires a diagonalizable square matrix, **SVD** works for any $m \times n$ matrix. It is the ultimate tool for data compression and dimensionality reduction.

### The Logic of SVD
For any matrix $A$, the matrix $A^T A$ is always **symmetric** and thus orthogonally diagonalizable. SVD uses this property to decompose $A$ into:
$$A = U \Sigma V^T$$
* **$U$** ($m \times m$): Orthogonal matrix of left singular vectors (eigenvectors of $AA^T$).
* **$\Sigma$** ($m \times n$): Diagonal matrix containing **singular values** ($\sigma_i = \sqrt{\lambda_i}$).
* **$V$** ($n \times n$): Orthogonal matrix of right singular vectors (eigenvectors of $A^T A$).

---
**References**

1. David C. Lay, Steven R. Lay, Judi J. McDonald, *Linear Algebra and Its Applications*, 5th ed.
2. Gilbert Strang, *Introduction to Linear Algebra*, 5th ed.
3. Stephen H. Friedberg, Arnold J. Insel, Lawrence E. Spence, *Linear Algebra*, 5th ed.
