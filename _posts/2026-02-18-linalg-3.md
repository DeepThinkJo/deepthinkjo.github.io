---
title: "Linear Algebra (3): Into the Computable Worlds"
date: 2026-02-18
categories: [Fundamentals, Linear Algebra]
tags: [math, linear-algebra, ai, ml, engineering, matrix, matrix-multiplication, inverse-matrix]
math: true
---

# Into the Computable Worlds

In the previous post, we confirmed through the concept of **Isomorphism** that any abstract vector can be transformed into a format that computers can handle: an **array of numbers**. This means we no longer need to struggle with the ambiguity of abstract vectors. We can now represent them as tangible, computable arrays. 

From this point forward, we will focus on vectors as 1-D arrays (in $\mathbb{R}^n$) and matrices as 2-D arrays (in $\mathbb{R}^{m \times n}$)—the very language computers speak.

---

## Deep Thoughts of Jo: Dimension of an Array?

If you have been following this series closely, you might have noticed a potential contradiction: 
*"Wait, didn't you say 'Dimension' is a property of a Vector Space, not a vector or a matrix? Why did you just call them 1-D and 2-D arrays?"*

This is a valid question. In Computer Science, we often use the term "dimension" for convenience to describe the structure of data, which differs slightly from the formal mathematical definition.

### Array in Computer Science
An **Array** is a data structure that stores elements of the same type in contiguous memory. To implement vectors and matrices in a computer, we use numeric data types (integers or floats).

- **Vector**: Usually implemented as a simple list: $[x_1, x_2, \dots, x_n]$.
- **Matrix**: Implemented as an "array of arrays" to distinguish rows: $[[x_{11}, \dots, x_{1n}], [x_{21}, \dots, x_{2n}], \dots]$.

In CS, because a matrix is nested twice (an array containing arrays), it is called a **2-dimensional array**. As we deal with higher-order data, such as **Tensors** (3-D arrays or higher), the "dimension" simply refers to the number of nesting levels (indices) required to access a single scalar.

---

## Matrix as the Representation of a Linear Transformation

A matrix is fundamentally a means to express a linear transformation between vector spaces. Every linear transformation $T: V \to W$ between finite-dimensional spaces can be uniquely represented by a matrix.

When $V = \mathbb{R}^n$ and $W = \mathbb{R}^m$, the matrix is not just a representation—it becomes the **operator** itself. If a linear transformation $T: \mathbb{R}^n \to \mathbb{R}^m$ is represented by a matrix $A$, then applying the transformation is equivalent to matrix-vector multiplication: $T(\mathbf{x}) = A\mathbf{x}$.

---

## Matrix Multiplication

To understand how transformations are applied, we must define **Matrix Multiplication**.

### Definition: Matrix Multiplication
Let $A$ be an $m \times p$ matrix and $B$ be a $p \times n$ matrix. The product $C = AB$ is an $m \times n$ matrix where the element in the $i$-th row and $j$-th column is given by:
$$C_{ij} = \sum_{k=1}^{p} A_{ik} B_{kj}$$

**Condition for Multiplication**: The number of columns in the first matrix ($A$) must equal the number of rows in the second matrix ($B$). This "inner dimension" match ($p$) is essential for the operation to be defined.

### Bridging Matrix-Matrix and Matrix-Vector Products
At this point, you might wonder: *"Isn't the definition above for the product of two matrices ($AB$), rather than the matrix-vector product ($Ax$) we discussed earlier?"* This is not a problem at all. Since we treat both vectors and matrices as arrays, a vector can simply be viewed as a specialized form of a matrix. To formalize this, we need to define vectors in terms of their matrix shapes.

### Definition: Row Vector and Column Vector
- **Row Vector**: A matrix with only one row ($1 \times n$).
- **Column Vector**: A matrix with only one column ($m \times 1$).

In most Linear Algebra and ML contexts, when we refer to a vector $\mathbf{x} \in \mathbb{R}^n$, we **implicitly assume it is a Column Vector** ($n \times 1$). Under this assumption, the product $A\mathbf{x}$ is perfectly consistent with the standard definition of matrix multiplication.

---

## Transpose

The **Transpose** is a fundamental operation that flips a matrix over its diagonal.

### Definition: Transpose
The transpose of an $m \times n$ matrix $A$, denoted $A^T$, is an $n \times m$ matrix where:
$$(A^T)_{ij} = A_{ji}$$
Effectively, the rows of $A$ become the columns of $A^T$.

---

## Special Matrices

Certain matrices possess structural properties that are extremely useful in engineering:

- **Square Matrix**: A matrix where the number of rows equals the number of columns ($n \times n$).
- **Upper Triangular Matrix**: A square matrix where $A_{ij} = 0$ for $i > j$.
- **Lower Triangular Matrix**: A square matrix where $A_{ij} = 0$ for $i < j$.
- **Diagonal Matrix**: A square matrix where $A_{ij} = 0$ for $i \neq j$.
- **Scalar Matrix**: A diagonal matrix where all diagonal entries are the same scalar $c$.
- **Identity Matrix ($I$)**: A scalar matrix where the diagonal entries are all 1.

**Note**: These definitions follow a hierarchy. For instance, an Identity matrix is a specific type of Scalar matrix, which in turn is a Diagonal matrix.

### Transpose-Related Matrices
- **Symmetric Matrix**: A matrix $A$ such that $A^T = A$.
- **Skew-Symmetric Matrix**: A matrix $A$ such that $A^T = -A$.
- **Orthogonal Matrix**: A square matrix $Q$ such that $Q^T Q = Q Q^T = I$. This implies $Q^T = Q^{-1}$.

---

## Inverse of a Matrix

The **Inverse** represents the process of "undoing" a linear transformation.

### Definition: Inverse of a Matrix
For a square matrix $A$, if there exists a matrix $B$ such that $AB = BA = I$, then $B$ is the **inverse** of $A$, denoted as $A^{-1}$.

### Definition: Invertible (Non-singular) Matrix
A matrix that has an inverse is called **invertible**. If no such inverse exists, it is called **singular**.

### Theorem
If a matrix $A$ is invertible, then $A$ must be a **square matrix**.

---

## Deep Thoughts of Jo: Transpose as a Domain-Codomain Swap

Earlier, we discussed the **Transpose** operation. Thinking about it deeply, I came to an interesting realization.

Consider an $m \times n$ matrix $A$ as a linear transformation $T: \mathbb{R}^n \to \mathbb{R}^m$. Its transpose, $A^T$, is an $n \times m$ matrix, which can be viewed as a transformation $T^T: \mathbb{R}^m \to \mathbb{R}^n$.

Essentially, the transpose operation "swaps" the roles of the domain and the codomain. While it is technically incorrect to say the transpose is always the inverse (since transposes don't usually "undo" the mapping, and non-square matrices aren't invertible), the relationship between them is fascinating. In the special case of **Orthogonal Matrices**, the transpose *is* the inverse!

The core takeaway is this: **The essence of a matrix is the linear transformation itself.** Once you view matrices as operators, concepts like transpose and inverse become much more than just arithmetic rules—they become structural insights.

---
**References**

1. Stephen H. Friedberg, Arnold J. Insel, Lawrence E. Spence, *Linear Algebra*, 5th ed.
2. Erwin Kreyszig, *Advanced Engineering Mathematics*, 10th ed.
3. David C. Lay, Steven R. Lay, Judi J. McDonald, *Linear Algebra and Its Applications*, 5th ed.
