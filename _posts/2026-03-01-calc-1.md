---
title: "Calculus (1): Limits and the Derivative (Single-Variable)"
date: 2026-03-01
categories: [Fundamentals, Calculus]
tags: [math, calculus, ai, ml, engineering, limits, derivatives, epsilon-delta]
math: true
---

## Introduction: The Bottom-Up Approach

To ultimately master the **Gradients** and **Jacobians** that power deep learning, we must follow a **bottom-up approach**. Mathematics is a cumulative discipline; we must build a solid foundation with simpler concepts before tackling high-dimensional optimization. 

This post focuses on the **Limit** and the **Derivative** of univariate (single-variable) functions. While most readers may have a high-school level understanding of these topics, we will revisit them with the mathematical rigor required for engineering, ensuring no logical gaps remain.

---

## Limits: From Intuition to Rigor

Before defining a derivative, we must understand the **Limit**. In essence, a derivative is a limit applied to the **rate of change**. While the intuitive meaning of a limit is the value a function approaches as its input gets closer to a certain point, engineering demands a more formal definition.

### The Formal Definition: $\epsilon-\delta$ Notation

For a function to be mathematically robust, we use the $\epsilon-\delta$ definition. This allows us to prove convergence by showing that we can make the output error ($\epsilon$) as small as we want by restricting the input range ($\delta$).



#### 1. Finite Limit at a Finite Point ($x \to a, f(x) \to L$)
We say

$$
\lim_{x \to a} f(x) = L
$$

if for every $\epsilon > 0$, there exists a $\delta > 0$ such that:

$$0 < |x - a| < \delta \implies |f(x) - L| < \epsilon$$

#### 2. Infinite Limits at a Finite Point ($x \to a, f(x) \to \pm\infty$)
* **Positive Infinity ($f(x) \to \infty$):** For every $M > 0$, there exists a $\delta > 0$ such that:

  $$
  0 < |x - a| < \delta \implies f(x) > M
  $$
  
* **Negative Infinity ($f(x) \to -\infty$):** For every $M < 0$, there exists a $\delta > 0$ such that:

  $$
  0 < |x - a| < \delta \implies f(x) < M
  $$

#### 3. Finite Limits at Infinity ($x \to \pm\infty, f(x) \to L$)
* **As $x \to \infty$:** For every $\epsilon > 0$, there exists an $N > 0$ such that:

  $$
  x > N \implies |f(x) - L| < \epsilon
  $$
  
* **As $x \to -\infty$:** For every $\epsilon > 0$, there exists an $N < 0$ such that:

  $$
  x < N \implies |f(x) - L| < \epsilon
  $$

#### 4. Infinite Limits at Infinity ($x \to \pm\infty, f(x) \to \pm\infty$)
* **$x \to \infty, f(x) \to \infty$:** $\forall M > 0, \exists N > 0$ s.t. $x > N \implies f(x) > M$
* **$x \to \infty, f(x) \to -\infty$:** $\forall M < 0, \exists N > 0$ s.t. $x > N \implies f(x) < M$
* **$x \to -\infty, f(x) \to \infty$:** $\forall M > 0, \exists N < 0$ s.t. $x < N \implies f(x) > M$
* **$x \to -\infty, f(x) \to -\infty$:** $\forall M < 0, \exists N < 0$ s.t. $x < N \implies f(x) < M$

---

## Derivatives: From Secant to Tangent

The **Derivative** is the mathematical tool for analyzing how a function changes. Letâ€™s build the intuition through geometry.

Consider a graph of a function. A line passing through two distinct points on this graph is a **Secant Line**. Its slope is the ratio of the increment in the dependent variable to the increment in the independent variable.



As we move one point closer to the other until the distance ($h$) between them approaches zero, the secant line becomes a **Tangent Line**. The slope of this tangent line is the **Derivative** at that specific point.

### Definition: Derivative
The derivative of a function $f$ at a number $a$, denoted by $f'(a)$, is defined as:
$$f'(a) = \lim_{h \to 0} \frac{f(a+h) - f(a)}{h}$$
If this limit exists, we say that $f$ is **differentiable** at $a$. The process of finding this derivative is called **Differentiation**.

---

## Differentiable Functions

Not every function is differentiable at every point. A function must be "smooth" enough for a unique tangent line to exist.

### Theorem: Differentiability and Continuity
There is a fundamental relationship between being differentiable and being continuous:
* **Theorem**: If a function $f$ is differentiable at $a$, then $f$ is continuous at $a$.
* **Note**: The converse is **not** necessarily true. A function can be continuous but not differentiable (e.g., $f(x) = \lvert x \rvert$ at $x=0$).

---

## Deep Thoughts of Jo: Differentiability of Activation Functions

In the context of Neural Networks, we frequently use the **ReLU (Rectified Linear Unit)** activation function:

$$
\text{ReLU}(x) = 
\begin{cases} 
x & \text{if } x > 0 \\
0 & \text{if } x \le 0 
\end{cases}
$$



Is ReLU differentiable? Technically, **no**, it is non-differentiable at $x=0$ because the left-hand limit of the slope (0) and the right-hand limit (1) are not equal.

However, in the engineering world of AI, this is not a deal-breaker. Deep learning frameworks like **PyTorch** use **sub-gradients**, effectively assigning a value (usually 0 or 1) at the point of non-differentiability. While we must be aware of this mathematical "kink," it rarely hinders optimization in practice. Maintaining this balance between mathematical scrutiny and engineering pragmatism is crucial for any AI researcher.

---

## References
- Stewart, J. (2012). *Essential Calculus: Early Transcendentals* (2nd ed.). Cengage Learning.
- Kochenderfer, M. J., & Wheeler, T. A. (2019). *Algorithms for Optimization*. MIT Press.
