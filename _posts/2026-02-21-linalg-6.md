---
title: "Linear Algebra (6): Eigenvalues and Eigenvectors"
date: 2026-02-21
categories: [Fundamentals, Linear Algebra]
tags: [math, linear-algebra, ai, ml, engineering, eigenvalues, eigenvectors, diagonalization, matrix-decomposition]
math: true
---

# Eigenvalues and Eigenvectors

To analyze a matrix deeply, we must view it as a **Linear Transformation**. Most vectors, when multiplied by a matrix $A$, will change both their magnitude and direction. However, there exist special vectors that stay on the same spanâ€”they only change their magnitude (scaling) without changing their direction. Finding these vectors and their scaling factors is the heart of the **Eigenvalue Problem**.

---

## Definition: Eigenvector and Eigenvalue

Let $A$ be an $n \times n$ matrix. A scalar $\lambda$ is called an **eigenvalue** of $A$ if there is a **non-zero vector** $\mathbf{x}$ such that:
$$A\mathbf{x} = \lambda \mathbf{x}$$
Such a vector $\mathbf{x}$ is called an **eigenvector** corresponding to $\lambda$.



### Why must the eigenvector be non-zero?
Mathematically, if $\mathbf{x} = \mathbf{0}$, the equation $A\mathbf{0} = \lambda \mathbf{0}$ holds true for any $\lambda$, yielding no useful information. 
From the perspective of **Information Theory**, a "surprising" or "specific" event carries more information. Since $\mathbf{x} = \mathbf{0}$ is a trivial solution that works for every matrix and every scalar, it contains **zero information**. To understand the unique properties of matrix $A$, we must find the **nontrivial solutions**.

---

## The Process: Finding $\lambda$ and $\mathbf{x}$

Solving the eigenvalue problem follows a systematic three-step process:

### 1. The Matrix Equation Form
We rewrite $A\mathbf{x} = \lambda \mathbf{x}$ as:
$$(A - \lambda I)\mathbf{x} = \mathbf{0}$$
While it might seem "tricky" to insert the identity matrix $I$, it is a logical necessity. We are searching for a linear transformation $A$ that acts like a simple scaling. Since every linear transformation in $\mathbb{R}^n$ can be represented as a matrix, we represent the scaling $\lambda$ as a **scalar matrix** $\lambda I$ (where the main diagonal entries are all $\lambda$).

### 2. The Characteristic Equation
We seek a nontrivial solution for $\mathbf{x}$. As we learned in previous posts, the homogeneous system $(A - \lambda I)\mathbf{x} = \mathbf{0}$ has a nontrivial solution if and only if the matrix $(A - \lambda I)$ is **non-invertible**.
$$\det(A - \lambda I) = 0$$
This is called the **Characteristic Equation**. The roots of the resulting **Characteristic Polynomial** are the eigenvalues of $A$.

### 3. Finding the Eigenspace
For each eigenvalue $\lambda$ found, we solve the linear system $(A - \lambda I)\mathbf{x} = \mathbf{0}$. The solution set (excluding the zero vector) provides the eigenvectors. This set of all solutions is called the **Eigenspace** of $A$ corresponding to $\lambda$.

---

## Eigenvalue Decomposition (Eigendecomposition)

The most powerful application of these concepts is breaking down a matrix into its fundamental components, often called **Diagonalization**.

### Definitions: Similarity and Diagonalizability
* **Similarity**: Matrix $A$ is similar to $B$ if there exists an invertible matrix $P$ such that $A = PBP^{-1}$. Similar matrices share the same eigenvalues.
* **Diagonalizable**: A square matrix $A$ is diagonalizable if it is similar to a diagonal matrix $D$.

### Theorems for Diagonalization
1.  An $n \times n$ matrix $A$ is diagonalizable **if and only if** it has $n$ linearly independent eigenvectors.
2.  If such eigenvectors exist, $A$ can be decomposed as:
    $$A = PDP^{-1}$$
    * Columns of $P$: The $n$ linearly independent eigenvectors.
    * Diagonal entries of $D$: The eigenvalues corresponding to the eigenvectors in $P$.

---

## Applications and Efficiency

### 1. Computing Matrix Powers ($A^k$)
In many engineering simulations, we need to compute $A^k$. Multiplying an $n \times n$ matrix $k$ times has a time complexity of roughly $O(k \cdot n^3)$, which is extremely expensive.
If $A$ is diagonalizable:
$$A^k = (PDP^{-1})(PDP^{-1})\dots(PDP^{-1}) = PD^kP^{-1}$$
Since $D$ is a diagonal matrix, $D^k$ is simply the main diagonal entries raised to the power of $k$. This reduces a massive matrix product into a simple scalar power operation.

### 2. The Foundation of AI: SVD and Beyond
In Machine Learning, **Singular Value Decomposition (SVD)** is used for data compression and noise reduction. Eigendecomposition is a core component of SVD. Mastering this allows us to understand how AI systems "analyze" data by breaking it down into its most significant directions (principal components).

---

## Deep Thoughts of Jo: Decomposition as Analysis

In science, we analyze complex objects by breaking them down into simpler parts. A matrix is a **Linear Transformation**, and **Matrix Multiplication** is the **Composition** of these transformations. 

Matrix Decomposition ($A = PDP^{-1}$) allows us to view one complex transformation as a sequence of three simpler steps:
1.  **$P^{-1}$**: Changing the coordinate system (basis).
2.  **$D$**: Scaling along the new axes.
3.  **$P$**: Changing the coordinates back to the original system.

If $P$ is **Orthogonal** (a concept we will explore in the next post), this can be geometrically interpreted as:
> **Rotate (without changing magnitude) $\to$ Scale (Shear/Stretch) $\to$ Rotate back.**

By decomposing a matrix, we aren't just doing math; we are dissecting the "DNA" of a linear transformation to see how it moves and shapes the space.

---
**References**

1. David C. Lay, Steven R. Lay, Judi J. McDonald, *Linear Algebra and Its Applications*, 5th ed.
2. Gilbert Strang, *Introduction to Linear Algebra*, 5th ed.
3. Stephen H. Friedberg, Arnold J. Insel, Lawrence E. Spence, *Linear Algebra*, 5th ed.
